{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#from sentence_transformers import CrossEncoder\n",
    "#from pytorch_modelsize import SizeEstimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import os\n",
    "\n",
    "tqdm.pandas()\n",
    "#model = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "import codecs\n",
    "import hickle as hkl\n",
    "import pickle\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_sentences(data, filename=\"sentences.pkl\",path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"): #saves data with a given filename in a path\n",
    "    if not os.path.exists(os.path.join(path,filename)):\n",
    "        print(data)\n",
    "        pickle.dump(data, open(os.path.join(path,filename), 'wb'))\n",
    "    else:\n",
    "        print(\"the file already exists\")\n",
    "\n",
    "def read_conversations(names=[]): #It reads a text file that contains subtitles referred to a conversation between 2 or more people. It expects that each time a person talks, we have\n",
    "                                  #one or more lines with the format name: sentence\n",
    "    #the person pronounced, while idx is an incremental number that is incremented each time a person talks\n",
    "    path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\\[English] 1.5 HOUR English Conversation Lesson [DownSub.com].txt\"\n",
    "    names = ['Vanessa', 'Dan']\n",
    "    sentences = dict((name,[]) for name in names)\n",
    "    #this dictionary has names of actors as keys, as value they have a tuple (sentence, idx) where sentence is the sentence that\n",
    "\n",
    "    with open(path, \"r\", encoding='utf-8') as f:\n",
    "        subs = [line.rstrip() for line in f if line.rstrip() != '']  #removes \\n and \"\\\" before apostroph\n",
    "        i = 0\n",
    "        turn = 1 # counter that counts the turns ( turn = turn + 1 each time a person finish to talk and the other starts)\n",
    "        while i<len(subs):\n",
    "            actual_name_list = [name for name in names if name+\":\" in subs[i]] #is the person that is currently talking\n",
    "            if len(actual_name_list) == 1: # A person is talking. if the person is the same in the next rows, the sentence is composed by all the the words in these rows put together\n",
    "                name = actual_name_list[0]\n",
    "                sentence = subs[i][len(actual_name_list[0])+1:]    #remove \"Name:\"\n",
    "                #print(i,len(subs)-1)\n",
    "                if i < len(subs)-1: #avoid exceeding the list of subtitles\n",
    "                    i = i + 1\n",
    "                    next_name_list = [name for name in names if name+\":\" in subs[i]]\n",
    "                    while len(next_name_list) == 0 and i != len(subs)-1 or (len(next_name_list)==1 and next_name_list[0] == name):  #same person is talking\n",
    "                        sentence = sentence + ' ' + subs[i]\n",
    "                        i = i + 1\n",
    "                        next_name_list = [name for name in names if name+\":\" in subs[i]]\n",
    "                        #print( len(next_name_list), next_name_list, name, i != len(subs), i , len(subs)-1)\n",
    "                    #print(\"OUT!\")\n",
    "\n",
    "                sentences[name].append((sentence,turn))\n",
    "                turn = turn + 1\n",
    "                #print(i,len(subs)-1)\n",
    "            else: # If we have for any reason more names or no names at the start of the sentence, we skip it\n",
    "                i = i+1\n",
    "    return sentences\n",
    "\n",
    "all_sentences = read_conversations()\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "save_sentences(all_sentences,path=path)\n",
    "filename = \"sentences.pkl\"\n",
    "data = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Subs_classification:\n",
    "\n",
    "    # filename contains the subtitles filepath\n",
    "    # refs contains the dictionary with all the reference sentences\n",
    "    # model variable contains the model that will be used for the semantic similarity\n",
    "    # winsize represents the number of words inside the sentence that will be used as comparison\n",
    "        # with the reference sentences. Not necessary in the \"fullstop_classification\" mode\n",
    "    # stride indicates the amount of words skipped to traslate the window. Not necessary in \"fullstop_regression\" mode\n",
    "    # thresold represents the value over which we consider the prediction of the model \"valid\", i.e. when\n",
    "        # reference sentence and sentence are similar enough\n",
    "    # mode is a string that represents the modality that we want to use as comparison:\n",
    "        # \"fullstop_regression\" compares a reference sentence with a sentence between two full stop\n",
    "        # \"window_regression\" compares a reference sentence with a sentence made up by the words inside the window\n",
    "        # \"mixed_rgression\" compares a reference sentence with a sentence made up by the words inside the window but\n",
    "            # it also find relations between windows inside a sentence between two full stops. The last window does not take\n",
    "            # the words after the full stop\n",
    "\n",
    "    def __init__(self, filename = '', refs = {},\n",
    "                 model = CrossEncoder('cross-encoder/stsb-roberta-base'),\n",
    "                 winsize = 5, stride = 2, threshold = 0.3, mode = \"fullstop_regression\"):\n",
    "\n",
    "        self.filepath = filename\n",
    "        self.subs = ''           #self.read_file(filename)\n",
    "        self.refs = refs\n",
    "        self.winsize = winsize\n",
    "        self.stride = stride\n",
    "        self.threshold = threshold\n",
    "        self.mode = mode\n",
    "\n",
    "    # used to read the \"filename\" file. It reads a row at time and then puts all the rows together\n",
    "    def read_file(self, filename):\n",
    "\n",
    "        with open(subtitles_path,\"r\",encoding='utf-8') as f:\n",
    "            subs = [line.rstrip() for line in f if line.rstrip()!='']  #removes \\n\n",
    "            subs_final = ''\n",
    "            #subs_final = [subs_final + s + ' ' for s in subs]\n",
    "            for el in subs:\n",
    "                subs_final = subs_final + el + ' '  #puts all the subs together\n",
    "            self.subs = subs_final\n",
    "            return self.subs\n",
    "\n",
    "\n",
    "    def sub_classifier(self, mode='', method='', sentences = '', type = 'conversation'):\n",
    "\n",
    "        if mode!='':\n",
    "            self.mode = mode\n",
    "        if method!='':\n",
    "            self.method = method\n",
    "\n",
    "        if self.mode == \"fullstop_regression\":\n",
    "            return self._fullstop_regression(self.method)\n",
    "        elif self.mode == \"window_regression\":\n",
    "            return self._window_regression(self.method)\n",
    "        elif self.mode == \"mixed_regression\":\n",
    "            return self._mixed_regression(self.method, sentences, type)\n",
    "        else:\n",
    "            print(\"Choose a right mode and a right method\")\n",
    "\n",
    "\n",
    "    def _append_results(self, thresholds_dict, value, ref, sentence):\n",
    "\n",
    "        thresholds_dict[\"value\"].append(value)\n",
    "        thresholds_dict[\"ref\"].append(ref)\n",
    "        thresholds_dict[\"sentence\"].append(sentence)\n",
    "\n",
    "        return thresholds_dict\n",
    "\n",
    "\n",
    "    def _best_threshold(self,local_th, all_th, sentence):\n",
    "\n",
    "        if len(local_th[\"value\"])!=0: #then we have at least one reference match\n",
    "            max_threshold = max(local_th[\"value\"])\n",
    "            index = local_th[\"value\"].index(max_threshold)\n",
    "            all_th = self._append_results(all_th, max_threshold,\n",
    "                                            local_th[\"ref\"][index], sentence)\n",
    "        #else:\n",
    "            #all_th = self._append_results(all_th, None,  None, sentence)\n",
    "        local_th = {\"value\":[], \"ref\":[], \"sentence\":[]} #I don't need the local thresholds anymore\n",
    "\n",
    "\n",
    "        return local_th, all_th\n",
    "\n",
    "    def _window_regression(self, method = \"best_threshold\"):\n",
    "\n",
    "        subs = self.read_file(self.filepath)    #all the subtitles\n",
    "        splits = subs.split()[0:200]\n",
    "        #best_th = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "        all_best_thresholds = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "        local_thresholds = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "        all_local_th = {}\n",
    "\n",
    "        for i in range(int(len(splits)/self.stride)):\n",
    "            sentence = \" \".join(splits[i*self.stride : i*self.stride + self.winsize]) #sentence that will be compared with all the ref sentences\n",
    "            for key in self.refs.keys():    #each key represents a topic related to a specific gesture, each element in the key is a sentence that can generate that gesture\n",
    "                for ref_sentence in self.refs[key]:   #for each reference, do a comparison with a sentence inside subtitles. If they are similar (>threshold) then append the result to local_thresholds\n",
    "                    res = model.predict([sentence,ref_sentence])\n",
    "                    if res > self.threshold:\n",
    "                        local_thresholds = self._append_results(local_thresholds, res, ref_sentence, sentence)\n",
    "\n",
    "                if method == \"best_threshold_for_each_topic\":     #for each topic, find the reference that is the most similar to the sentence (if there is anyone) and put results in all_best_thresholds\n",
    "                    local_thresholds, all_best_thresholds = self._best_threshold(local_thresholds, all_best_thresholds, sentence)\n",
    "\n",
    "            if method == \"best_threshold\":  #for each window, find the best reference of the best topic that is the most similar to the sentence (if there is anyone\n",
    "                local_thresholds, all_best_thresholds = self._best_threshold(local_thresholds, all_best_thresholds, sentence)\n",
    "\n",
    "        if method == \"best_threshold\" or method == \"best_threshold_for_each_topic\":\n",
    "            return all_best_thresholds\n",
    "        elif method == \"all_thresholds\":\n",
    "            return local_thresholds\n",
    "\n",
    "\n",
    "    def _fullstop_regression(self, method = \"best_threshold\"):\n",
    "\n",
    "        subs = self.read_file(self.filepath)\n",
    "        splits = subs.split(\".\")[0:200]\n",
    "        all_best_thresholds = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "        local_thresholds = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "\n",
    "        for sentence in splits:\n",
    "            for key in self.refs.keys():\n",
    "                for ref_sentence in self.refs[key]:\n",
    "                    res = model.predict([sentence,ref_sentence])\n",
    "                    if res > self.threshold:\n",
    "                        local_thresholds = self._append_results(local_thresholds, res, ref_sentence, sentence)\n",
    "\n",
    "                if method == \"best_threshold_for_each_topic\":     #for each topic, find the reference that is the most similar to the sentence (if there is anyone) and put results in all_best_thresholds\n",
    "                    local_thresholds, all_best_thresholds = self._best_threshold(local_thresholds, all_best_thresholds, sentence)\n",
    "\n",
    "            if method == \"best_threshold\":  #for each window, find the best reference of the best topic that is the most similar to the sentence (if there is anyone\n",
    "                local_thresholds, all_best_thresholds = self._best_threshold(local_thresholds, all_best_thresholds, sentence)\n",
    "\n",
    "        if method == \"best_threshold\" or method == \"best_threshold_for_each_topic\":\n",
    "            return all_best_thresholds\n",
    "        elif method == \"all_thresholds\":\n",
    "            return local_thresholds\n",
    "\n",
    "\n",
    "    def _mixed_regression(self, method = \"best_threshold\", sentences='', type = 'conversation'):\n",
    "        #TODO save the starting index of the matched words to avoid later correction\n",
    "        #subs = self.read_file(self.filepath)\n",
    "        subs = sentences\n",
    "        all_local_all_sentences=[]  #it contains all the local results for all possible windows for all the sentences\n",
    "        all_best_all_sentences=[]  #it contains all the local results for all possible windows for all the sentences\n",
    "        #print(subs)\n",
    "        if type == 'conversation':      # In this case sentences are of type (speaker,sentence,turn), otherwise we have only a list of sentences\n",
    "            conv_subs=[]\n",
    "            for key in list(subs.keys()):\n",
    "                speaker = (key,)\n",
    "                tuples = [speaker + element for element in subs[key]]\n",
    "                conv_subs.append(tuples)\n",
    "                #subs[key] = [ for (sentence,turn) in subs[key]]\n",
    "            #subs = conv_subs[0][0:10] + conv_subs[1][0:10] #take 10 sentences for 2 speakers\n",
    "            subs = conv_subs[1][:100] + conv_subs[0][:100]\n",
    "            #print(subs)\n",
    "\n",
    "        #print(subs)\n",
    "        #subs = subs[0:1] + subs[2:3] #+ subs[8:13]\n",
    "        #print(subs)\n",
    "        sentences_processed = 0\n",
    "        filename_best = \"all_processed_sentences_best.pkl\"\n",
    "        filename_local = \"all_processed_sentences_local.pkl\"\n",
    "        window_max_len = 10\n",
    "        path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "\n",
    "        print(\"loading file with local matches...\")\n",
    "        if(os.path.exists(os.path.join(path,filename_best))):    #load data already processed\n",
    "            all_best_all_sentences = pickle.load(open(os.path.join(path,filename_best),'rb'))\n",
    "        else:\n",
    "            pickle.dump(all_best_all_sentences, open(os.path.join(path,filename_best),'wb'))\n",
    "        print(\"File with local matches loaded! Loading now the file with best matches...\")\n",
    "        if(os.path.exists(os.path.join(path,filename_local))):    #load data already processed\n",
    "            all_local_all_sentences = pickle.load(open(os.path.join(path,filename_local),'rb'))\n",
    "        else:\n",
    "            pickle.dump(all_local_all_sentences, open(os.path.join(path,filename_local),'wb'))\n",
    "        print(\"both files loaded\")\n",
    "        #print(all_best_all_sentences)\n",
    "\n",
    "        for sentence in subs:\n",
    "            all_local_th = {}\n",
    "            all_best_th = {}\n",
    "            best_th = {\"value\":[], \"ref\":[], \"sentence\":[]}\n",
    "            local_th = {\"value\":[], \"ref\":[], \"sentence\":[]}    #contains all the possible matches for a given winsize\n",
    "            all_locals = {\"value\":[], \"ref\":[], \"sentence\":[]}  #contains all the possible matches for a given winsize, but differently from local_th, it\n",
    "                                                                #is does not become empty if method is \"best_threshold\" or \"best_threshold_for_each_topic\"\n",
    "            window_size = self.winsize\n",
    "            skip_flag = False\n",
    "            if type == 'conversation':\n",
    "                #print(sentence,len(sentence))\n",
    "                speaker = sentence[0]\n",
    "                turn = sentence[2]\n",
    "                sentence = sentence[1]\n",
    "                for i in range(len(all_best_all_sentences)):\n",
    "                    if((sentence,speaker,turn) == all_best_all_sentences[i][\"whole sentence,speaker,turn\"]): #we already have this data\n",
    "                        skip_flag = True\n",
    "                        print(f\"skipped: {(sentence,speaker,turn)}\")\n",
    "                        sentences_processed = sentences_processed + 1\n",
    "                if skip_flag == True:\n",
    "                        continue\n",
    "                else:\n",
    "                    all_local_th[\"whole sentence,speaker,turn\"] = (sentence,speaker,turn)\n",
    "                    all_best_th[\"whole sentence,speaker,turn\"] = (sentence,speaker,turn)\n",
    "            else:\n",
    "                for i in range(len(all_best_all_sentences)):\n",
    "                    if(sentence == all_best_all_sentences[i][\"whole sentence\"]):\n",
    "                        skip_flag = True\n",
    "                        print(f\"skipped: {sentence}\")\n",
    "                        sentences_processed = sentences_processed + 1\n",
    "                        continue\n",
    "                if skip_flag == True:\n",
    "                    continue\n",
    "                else:\n",
    "                    all_local_th[\"whole sentence\"] = sentence\n",
    "                    all_best_th[\"whole sentence\"] = sentence\n",
    "\n",
    "            sentence_splits = sentence.split()\n",
    "            while window_size <= len(sentence_splits) and window_size <= window_max_len:\n",
    "                #print(window_size)\n",
    "                #print(self.winsize)\n",
    "                #print(f\"HERE:{sentence_splits}\")\n",
    "                #print(len(sentence_splits))\n",
    "                n_windows = int((len(sentence_splits) - window_size + 1) / self.stride)\n",
    "                #print(n_windows)\n",
    "                for i in range(n_windows):  #use the window inside the sentence\n",
    "                    sub_sentence = \" \".join(sentence_splits[i*self.stride : i*self.stride + window_size]) #sentence that will be compared with all the ref sentences\n",
    "                    for key in self.refs.keys():    #for all types of references\n",
    "                        for ref_sentence in self.refs[key]: #for all references\n",
    "                            #print(self.winsize, sub_sentence)\n",
    "                            res = model.predict([sub_sentence,ref_sentence])\n",
    "                            if res > self.threshold:    #then we have a match\n",
    "                                local_th = self._append_results(local_th, res, ref_sentence, sub_sentence) #put the result in local_th\n",
    "                                all_locals = self._append_results(all_locals, res, ref_sentence, sub_sentence) #same operation but this list do not become empty if method is \"best_threshold_for_each_topic\" or \"best_threshold\". Used for saving purposes\n",
    "\n",
    "                        if method == \"best_threshold_for_each_topic\":     #for each topic, find the reference that is the most similar to the sentence (if there is anyone) and best_th_topic\n",
    "                            local_th, best_th = self._best_threshold(local_th, best_th, sub_sentence)\n",
    "\n",
    "                    if method == \"best_threshold\":  #for each window, find the best reference of the best topic that is the most similar to the sentence (if    there is anyone)\n",
    "                        local_th, best_th = self._best_threshold(local_th, best_th, sub_sentence)\n",
    "                        #print(best_th)\n",
    "\n",
    "                all_local_th[f\"window {window_size}\"] = all_locals  #save results for each window size\n",
    "                all_best_th[f\"window {window_size}\"] = best_th\n",
    "                local_th = {\"value\":[], \"ref\":[], \"sentence\":[]} #I don't need the local thresholds anymore\n",
    "                best_th = {\"value\":[], \"ref\":[], \"sentence\":[]} #I don't need the best thresholds anymore\n",
    "                window_size = window_size + 1\n",
    "            #print(all_best_th)\n",
    "            all_local_all_sentences.append(all_local_th)\n",
    "            all_best_all_sentences.append(all_best_th)\n",
    "            sentences_processed = sentences_processed + 1\n",
    "            print(\"writing to file...\")\n",
    "            pickle.dump(all_best_all_sentences, open(os.path.join(path,filename_best),'wb'))\n",
    "            pickle.dump(all_local_all_sentences, open(os.path.join(path,filename_local),'wb'))\n",
    "            print(f\"file written {sentence, speaker, turn}, sentence processed: {sentences_processed}\")\n",
    "            #print(f\"processed a sentence:{all_best_th}\")\n",
    "            #print(all_local_th)\n",
    "\n",
    "\n",
    "        if method == \"best_threshold\" or method == \"best_threshold_for_each_topic\":\n",
    "            return all_best_all_sentences\n",
    "        elif method == \"all_thresholds\":\n",
    "            return all_local_all_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "th = 0\n",
    "filename = \"sentences.pkl\"\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "all_sentences = pickle.load(open(os.path.join(path,filename),'rb'))\n",
    "classifier = Subs_classification(filename = subtitles_path, refs = all_refs, winsize = 1, stride = 1,\n",
    "                                  threshold = th, mode = \"mixed_regression\")\n",
    "result = classifier.sub_classifier(mode='', method=\"best_threshold_for_each_topic\", sentences = all_sentences , type = 'conversation')\n",
    "#print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#reference sentences that will be compared by the model with a sentence inside subtitles\n",
    "\n",
    "reference_sentence_greet = [\"hello\", \"hey there\", \"I greet you\", \"Greetings!\", \"Namaste\", \"good day\", \"goodbye\", \"see you\"]\n",
    "reference_sentence_faster = [\"this is boring, go faster\", \"please talk faster\", \"continue please\", \"I have no time, finish the speech please\"]\n",
    "reference_sentence_goaway = [\"let me alone\", \"do not disturb me\", \"I need some privacy\", \"go away\", \"you annoyed me\"]\n",
    "reference_sentence_cigarette = [\"do you like to smoke?\", \"smoking is not healthy\", \"cigarette\", \"where are cigarettes\",\n",
    "                                \"smoking is the cause of many diseases\", \"I like to smoke\", \"smoking\"]\n",
    "reference_sentence_idk = [\"I don't know\", \"I don't understand\"]\n",
    "reference_sentence_no = [\"I don't agree\", \"I don't like it\", \"no\", \"I don't want\", \"it's not ok\", \"It is not the same for me\"]\n",
    "reference_sentence_yes = [\"yes.\", \"yeah.\", \"I agree\", \"It is the same for me\", \"I like it\", \"ok\", \"I want it\"]\n",
    "reference_sentence_deitic = [\"see there\", \"see that\", \"look at your left\", \"look at your right\", \"look over your head\", \"look behind\"]\n",
    "reference_sentence_stop = [\"slow down\", \"don't go further\", \"stop\", \"go slower\", \"stop talking\", \"talk slower\"]\n",
    "reference_sentence_adaptor = [\"my arm\", \"my hand\", \"my head\",\"my knee\", \"my leg\", \"my stomach\", \"my chest\",\n",
    "                             \"my feet\", \"my eyes\", \"me\", \"I am\"]\n",
    "reference_sentence_telephone = [\"I have to do a call\", \"I want to call my friend\", \"telephone number\", \"call me\"]\n",
    "reference_sentence_come = [\"come to me\", \"come here\", \"come closer\"]\n",
    "reference_sentence_drink = [\"do you want a drink?\", \"I want to drink\", \"I want some water\", \"can you give me a drink?\",\n",
    "                            \"let's take a drink\", \"I am thirsty\",\"let's take a drink\"]\n",
    "reference_sentence_exulting = [\"I won!\", \"I am very happy!\"]\n",
    "reference_sentence_link = [\"they are a couple\", \"they are lovers\", \"they are similar\", \"they understand each other\"]\n",
    "reference_sentence_apologize = [\"excuse me\", \"please forgive me\", \"I am sorry\"]\n",
    "reference_sentence_attention = [\"pay attention\", \"listen me\", \"look at here\"]\n",
    "reference_sentence_beg = [\"please listen me\", \"I beg you\"]\n",
    "reference_sentence_careless = [\"I don't care\", \"It's not my problem\"]\n",
    "reference_sentence_impossible = [\"How is it possible?\", \"It's impossible\"]\n",
    "reference_sentence_walk= [\"I like to walk\", \"do you like to walk?\", \"I like trekking\", \"I want to do some trekking\",\n",
    "                           \"I like hiking\", \"I often go out for a walk\"]\n",
    "reference_sentence_run = [\"I like to run\",\"do you like to run?\", \"where you will run?\", \"I often run\", \"I want to run later\"]\n",
    "reference_sentence_thin = [\"that person is thin\", \"I'm thin\", \"that object is thin\"]\n",
    "reference_sentence_later = [\"let's meet later\", \"see you tomorrow\", \"next week\", \"next month\", \"next year\", \"do it later\"]\n",
    "reference_sentence_lot = [\"it is very\", \"there is a lot\", \"it is too much\"]\n",
    "reference_sentence_time = [\"what time is it?\", \"can you tell me the time\", \"It's late\", \"you are late\"]\n",
    "reference_sentence_clap = [\"you did very well!\", \"compliments!\", \"congratulations!\"]\n",
    "reference_sentence_ita = [\"What do you want?\", \"what are you saying?\"]\n",
    "\n",
    "\n",
    "all_refs = {\"r_greet\":reference_sentence_greet, \"r_faster\":reference_sentence_faster, \"r_goaway\":reference_sentence_goaway,\n",
    "           \"r_cigarette\":reference_sentence_cigarette, \"r_idk\":reference_sentence_idk, \"r_no\":reference_sentence_no,\n",
    "           \"r_yes\":reference_sentence_yes, \"r_deitic\":reference_sentence_deitic,\"r_run\":reference_sentence_run,\n",
    "           \"r_stop\":reference_sentence_stop, \"r_adaptor\":reference_sentence_adaptor, \"r_telephone\":reference_sentence_telephone,\n",
    "           \"r_come\":reference_sentence_come, \"r_drink\":reference_sentence_drink, \"r_exulting\":reference_sentence_exulting,\n",
    "            \"r_link\":reference_sentence_link, \"r_apologize\":reference_sentence_apologize, \"r_attention\":reference_sentence_attention,\n",
    "            \"r_beg\":reference_sentence_beg, \"r_careless\":reference_sentence_careless, \"r_impossible\":reference_sentence_impossible,\n",
    "            \"r_walk\":reference_sentence_walk,\"r_thin\":reference_sentence_thin, \"r_later\":reference_sentence_later,\n",
    "            \"r_lot\":reference_sentence_lot, \"r_time\":reference_sentence_time, \"r_clap\":reference_sentence_clap,\"r_ita\":reference_sentence_ita}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def avg_windows(results, all_refs, new_th=0):\n",
    "\n",
    "    average_windows = {} #it will contain the average window for each topic, the standard deviation, the number of matches, the average and std of the values of the matches\n",
    "    all_topics_data = {} #it contains all the window sizes and all the values for each topic so that we can compute sum and std at the end and put the result in the previous dict\n",
    "    for topic in all_refs: #init the average windows\n",
    "        average_windows[topic] = {'avg_window':0, 'std_window':0, 'n_matches':0, 'avg_value':0, 'std_value':0}\n",
    "        all_topics_data[topic] = {'windows':[], 'values':[]}\n",
    "    for i,res in enumerate(results):+\n",
    "        sentence = res['whole sentence,speaker,turn'][0]\n",
    "        #speaker = res['whole sentence,speaker,turn'][1]\n",
    "        #turn = res['whole sentence,speaker,turn'][2]\n",
    "        window_dim = int(list(res.keys())[-1][-1]) #the last key of res contains data of the largest window that was used for the sentence. The last value of the key is window dim\n",
    "        #window_dim = len(sentence_splits)\n",
    "        #print(window_dim)\n",
    "        for j in range(1,window_dim+1):\n",
    "            values = res[f\"window {j}\"]['value']\n",
    "            th_indexes = [i for i in range(len(values)) if values[i] > new_th] #returns the indexes of the values that are greater than new_th\n",
    "            tmp_values = list(map(values.__getitem__,th_indexes)) #take all the values greater than new_th\n",
    "            tmp_ref = list(map(res[f\"window {j}\"]['ref'].__getitem__,th_indexes)) #take all the references that returns a value greater than new_th\n",
    "            #tmp_sentence = list(map(res[f\"window {j}\"]['sentence'].__getitem__,th_indexes))\n",
    "            for k,value in enumerate(tmp_values): #for each value that is greater than the threshold\n",
    "                ref = tmp_ref[k]\n",
    "                topic = [key for key in all_refs.keys() for t_ref in all_refs[key] if ref == t_ref][0] #extract the topic associated with reference\n",
    "                average_windows[topic]['n_matches'] = average_windows[topic]['n_matches'] + 1 #increment the number of matches\n",
    "                all_topics_data[topic]['windows'].append(j)\n",
    "                all_topics_data[topic]['values'].append(value)\n",
    "\n",
    "    for topic in all_refs:  #now we can compute all the averages and standard deviations\n",
    "        if average_windows[topic]['n_matches'] != 0:\n",
    "            average_windows[topic]['avg_window'] = round(np.mean(np.array(all_topics_data[topic]['windows'])),3)\n",
    "            average_windows[topic]['std_window'] = round(np.std(np.array(all_topics_data[topic]['windows'])),3)\n",
    "            average_windows[topic]['avg_value'] = round(np.mean(np.array(all_topics_data[topic]['values'])),3)\n",
    "            average_windows[topic]['std_value'] = round(np.std(np.array(all_topics_data[topic]['values'])), 3)\n",
    "\n",
    "    return average_windows\n",
    "\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "filename_best = \"all_processed_sentences_best.pkl\"\n",
    "filename_local = \"all_processed_sentences_local.pkl\"\n",
    "\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_best))):    #load data already processed\n",
    "    results = pickle.load(open(os.path.join(path,filename_best),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "\n",
    "average_windows = avg_windows(results, all_refs, new_th = 0.15)\n",
    "for i,el in enumerate(list(average_windows.keys())):\n",
    "    average_windows[el]['priority'] = np.random.randint(1000) #add a priority to the topic (now they are random, the order has yet to be decided)\n",
    "\n",
    "filename_res = \"average_windows_200_sentences.pkl\"\n",
    "print(\"writing to file... \\n \\n\")\n",
    "pickle.dump(average_windows , open(os.path.join(path,filename_res),'wb'))\n",
    "\n",
    "for el in list(average_windows.keys()):\n",
    "    print(f\"{el}: {average_windows[el]}\", \"\\n\", \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i,el in enumerate(list(res.keys())):\n",
    "    res[el]['priority'] = np.random.randint(1000) #add a priority to the topic (now they are random, the order has yet to be decided)\n",
    "topics_ordered = sorted(res.items(), key=lambda x: x[1]['priority'], reverse=False) #order the topics on priority values\n",
    "#print(topics_ordered[0][1]['avg_window'])\n",
    "\n",
    "n_possible_topics = 3\n",
    "index = 5\n",
    "a = range(0,1000)\n",
    "b = np.random.randint(1000,size=1000)\n",
    "values = [i for i in b[index : index + n_possible_topics]]\n",
    "indexes = [i for i in a[index : index + n_possible_topics]]\n",
    "#for i,j in zip(values,indexes):\n",
    "    #print(i,j)\n",
    "\n",
    "\n",
    "a = [0,2,3,4,5,8]\n",
    "b = [a[i]]\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# give priority to each topic. Use the average window of each topic to take data of the sentence. If, for a topic, we have overlapping windows, chose the one that returns the best value.\n",
    "# After we choose a topic given the value, the window size and the priority, all the words of that window can't be assigned to any other topic\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "filename_best = \"all_processed_sentences_best.pkl\"\n",
    "filename_local = \"all_processed_sentences_local.pkl\"\n",
    "filename_windows = \"average_windows_200_sentences.pkl\"\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_best))):    #load data already processed\n",
    "    results = pickle.load(open(os.path.join(path,filename_best),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_windows))):    #load data already processed\n",
    "    average_windows = pickle.load(open(os.path.join(path,filename_windows),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "\n",
    "\n",
    "def process_results_priority_policy_avg_windows(avg_windows = average_windows ,result = results, new_th=0.8):\n",
    "    final_labels=[]\n",
    "    for el in result:\n",
    "        sentence_labels = {'sentence':'', 'topics&ref':[], 'matching_words':[], 'values':[], 'priority_levels':[]} #it contains a sentence labelled\n",
    "        sentence = el['whole sentence,speaker,turn'][0]\n",
    "        speaker = el['whole sentence,speaker,turn'][1]\n",
    "        turn = el['whole sentence,speaker,turn'][2]\n",
    "        sentence_labels['sentence'] = sentence\n",
    "        sentence_splits = sentence.split()\n",
    "        final_word_index = len(sentence_splits)\n",
    "        current_word_index = 0\n",
    "        break_flag = False #if true,  we found the right label to a set of words in the sentence, so we can continue\n",
    "        topics_ordered = sorted(avg_windows.items(), key=lambda x: x[1]['priority'], reverse=True) #order the topics on priority values\n",
    "        topic_index = 0\n",
    "        while topic_index < len(topics_ordered) and current_word_index <= final_word_index:\n",
    "            topic_name = topics_ordered[topic_index][0]\n",
    "            window_dim = round(topics_ordered[topic_index][1]['avg_window'])\n",
    "            if current_word_index + window_dim > final_word_index: #we can't try with this topic since we reached the end of the sentence\n",
    "                topic_index = topic_index + 1\n",
    "                continue\n",
    "            words_in_window = ' '.join(sentence_splits[current_word_index : current_word_index + window_dim])\n",
    "            current_values = el[f\"window {window_dim}\"]['value']\n",
    "            th_indexes = [i for i in range(len(current_values )) if current_values[i] > new_th] #returns the indexes of the values that are greater than new_th\n",
    "            tmp_values = list(map(current_values.__getitem__,th_indexes)) #take all the values greater than new_th\n",
    "            tmp_ref = list(map(el[f\"window {window_dim}\"]['ref'].__getitem__,th_indexes)) #take all the references that return a value greater than new_th\n",
    "            tmp_sentences = list(map(el[f\"window {window_dim}\"]['sentence'].__getitem__,th_indexes)) #take all the sentence parts that return a value greater than new_th\n",
    "            #print(words_in_window, tmp_sentences, tmp_values)\n",
    "            if words_in_window in tmp_sentences: #then we have a match with a topic and the words in the window\n",
    "                #win_results = el[f\"window {window_dim}\"] #all the results that we have for a given window dimension\n",
    "                #values = win_results['value']   #all the values referred to the topics that return result greater than a threshold\n",
    "                index = tmp_sentences.index(words_in_window) #since we have at least one topic > threshold, we take the first index of the first topic that match\n",
    "                n_possible_topics = tmp_sentences.count(words_in_window) #all the possible topics that returns a result greater than threshold\n",
    "                all_indexes = [i for i in range(index, index + n_possible_topics)]\n",
    "                all_best_values = tmp_values[index : index + n_possible_topics] #all the values of the topics that match with the words in the window\n",
    "                for value,index in zip(all_best_values,all_indexes): #check if the current topic has a match. If no, skip it and go to the next one by following the priorities\n",
    "                    ref = tmp_ref[index] #reference associated to the result\n",
    "                    tmp_topic = [key for key in all_refs.keys() for tmp_ref in all_refs[key] if ref == tmp_ref][0] #extract the topic associated with reference\n",
    "                    if topic_name != tmp_topic: #check if another topic is greater than new_th\n",
    "                        continue\n",
    "                    else:\n",
    "                        sentence_labels['topics&ref'].append((topic_name,ref)) #save the result\n",
    "                        sentence_labels['matching_words'].append(words_in_window)\n",
    "                        sentence_labels['values'].append(value)\n",
    "                        sentence_labels['priority_levels'].append(topic_index+1)\n",
    "                        current_word_index = current_word_index + window_dim + 1\n",
    "                        break_flag = True\n",
    "                        break #break the loop and restart with another topic\n",
    "                topic_index = topic_index + 1 #restart from next topic\n",
    "                if break_flag == True:\n",
    "                    topic_index = 0 #we found the topic, restart the topic search from the current_word_index and reset the break_flag\n",
    "                    break_flag = False\n",
    "            else:\n",
    "                topic_index = topic_index + 1\n",
    "                if topic_index >= len(topics_ordered):\n",
    "                    current_word_index = current_word_index + 1\n",
    "                    topic_index = 0\n",
    "                continue #try with another topic. Changing topic means that we change the size of window so we take different words to compare. If no topics match, go to the next word\n",
    "        final_labels.append(sentence_labels)\n",
    "    return final_labels\n",
    "\n",
    "\n",
    "labels = process_results_priority_policy_avg_windows(avg_windows=average_windows,result=results, new_th=.7)\n",
    "filename_labels_avg = \"results_priority_policy_avg_windows.pkl\"\n",
    "print(\"writing to file...\")\n",
    "pickle.dump(labels  , open(os.path.join(path,filename_labels_avg),'wb'))\n",
    "\n",
    "for label in labels:\n",
    "    print(label, \"\\n \\n \\n \\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = list(np.random.randint(1000,size=100))\n",
    "b = [(a,i) for i in range(len(a))]\n",
    "ordered_res = sorted(a,reverse=True)\n",
    "final_res = [(ordered_res[i],a.index(ordered_res[i])) for i in range(len(a))]\n",
    "#print(final_res)\n",
    "#print([heapq.nlargest(len(a) , a)[i], a.index())\n",
    "c = []\n",
    "print(sorted(c), max(c))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "################################### TEST\n",
    "def adec(fun):\n",
    "    def wrapper(b):\n",
    "        res = fun(b)\n",
    "        res = res + 1\n",
    "        return res\n",
    "    return wrapper\n",
    "\n",
    "@adec\n",
    "def f(c):\n",
    "    return c+1\n",
    "print(f(3))\n",
    "'''\n",
    "def a_decorator_passing_arbitrary_arguments(function_to_decorate):\n",
    "    def a_wrapper_accepting_arbitrary_arguments(*args,**kwargs):\n",
    "        print('The positional arguments are', args)\n",
    "        print('The keyword arguments are', kwargs)\n",
    "        function_to_decorate(*args)\n",
    "    return a_wrapper_accepting_arbitrary_arguments\n",
    "\n",
    "@a_decorator_passing_arbitrary_arguments\n",
    "def function_with_no_argument():\n",
    "    print(\"No arguments here.\")\n",
    "\n",
    "function_with_no_argument()\n",
    "\n",
    "def decorator_maker_with_arguments(decorator_arg1, decorator_arg2, decorator_arg3):\n",
    "    def decorator(func):\n",
    "        def wrapper(function_arg1, function_arg2, function_arg3) :\n",
    "            \"This is the wrapper function\"\n",
    "            print(\"The wrapper can access all the variables\\n\"\n",
    "                  \"\\t- from the decorator maker: {0} {1} {2}\\n\"\n",
    "                  \"\\t- from the function call: {3} {4} {5}\\n\"\n",
    "                  \"and pass them to the decorated function\"\n",
    "                  .format(decorator_arg1, decorator_arg2,decorator_arg3,\n",
    "                          function_arg1, function_arg2,function_arg3))\n",
    "            return func(function_arg1, function_arg2,function_arg3)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "pandas = \"Pandas\"\n",
    "@decorator_maker_with_arguments(pandas, \"Numpy\",\"Scikit-learn\")\n",
    "def decorated_function_with_arguments(function_arg1, function_arg2,function_arg3):\n",
    "    print(\"This is the decorated function and it only knows about its arguments: {0}\"\n",
    "           \" {1}\" \" {2}\".format(function_arg1, function_arg2,function_arg3))\n",
    "\n",
    "decorated_function_with_arguments(pandas, \"Science\", \"Tools\")\n",
    "'''\n",
    "s = \"Hello, how are you?\"\n",
    "ind=1\n",
    "w_size=1\n",
    "s_split = s.split()\n",
    "print(s_split)\n",
    "new_s = ' '.join(s_split[(ind+1):(ind+1+w_size)])\n",
    "print(new_s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "filename_best = \"all_processed_sentences_best.pkl\"\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_best))):    #load data already processed\n",
    "    results = pickle.load(open(os.path.join(path,filename_best),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "\n",
    "def adjust_results(results, path ='', new_filename='', dim_max=10):   #current version works only if results are computed with treshold = 0 (we always have a match for each window) and                                                                      #stride = 1\n",
    "    count = 0\n",
    "    for key in all_refs.keys():\n",
    "        count = count + 1\n",
    "    for res in results:\n",
    "        sentence = res['whole sentence,speaker,turn'][0]\n",
    "        s_split = sentence.split()\n",
    "        #print(s_split, count)\n",
    "        d_max = len(s_split) if len(s_split) <= dim_max else dim_max\n",
    "        start_word_indexes = []\n",
    "        dim = 1\n",
    "        while dim <= d_max:\n",
    "            idx = 0 #dim - 1\n",
    "            while idx < len(s_split) - dim + 1:\n",
    "                #tmp_res = res[f\"window {dim}\"]\n",
    "                #print(tmp_res)\n",
    "                for counts in range(count):\n",
    "                    start_word_indexes.append(idx)\n",
    "                idx = idx + 1\n",
    "            res[f\"window {dim}\"]['start_word_indexes'] = start_word_indexes\n",
    "            start_word_indexes = []\n",
    "            #print(len(res[f\"window {dim}\"]['value']),len(res[f\"window {dim}\"]['start_word_indexes']), dim)\n",
    "            assert(len(res[f\"window {dim}\"]['value']) == len(res[f\"window {dim}\"]['start_word_indexes']))\n",
    "            dim = dim + 1\n",
    "            #last_word_indexes = []\n",
    "            #print(len(s_split) - dim + 1, len(res[f\"window {dim}\"]['value']),len(res[f\"window {dim}\"]['start_word_indexes']))\n",
    "    #print(\"writing to file...\")\n",
    "    #pickle.dump(results , open(os.path.join(path,new_filename),'wb'))\n",
    "    print(results)\n",
    "\n",
    "adjust_results([results[0]], path = path, new_filename = \"new_all_processed_sentences_best.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b = [0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4]\n",
    "res = [i for i in range(len(b)) if b[i]>2][0]\n",
    "print(res-1)\n",
    "c = [0,1,2,3]\n",
    "c[1:1+3]\n",
    "topics = {'t':[(1,3),(4,5),(2,6)]}\n",
    "if (1,3) in topics['t']:\n",
    "    print(\"Hello\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "filename_best = \"new_all_processed_sentences_best.pkl\"\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_best))):    #load data already processed\n",
    "    results = pickle.load(open(os.path.join(path,filename_best),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "\n",
    "# this function process the results obtained previously that contain all the possible matches between the words inside each sentence and all the references (grouped by topic, or gesture)\n",
    "# by using the sentence similarity. The function must have the following characteristics:\n",
    "# 1) only the results greater than a certain threhsold new_th are accepted\n",
    "# 2) in case we have a match between words inside a window and more than one references related to one topic, we take only the best one related to that topi (already true in\n",
    "# all_processed_sentences_best.pkl file)\n",
    "# 3) in case we have more topics that have a match with words in a window, we take the topic related to the best match\n",
    "# 4) the functions has to start with a window_dim equal to 1, then compares the best match of a topic with window one, with the matches found by incrementing the window size: it takes\n",
    "# the best one\n",
    "# 5) once we found the best window that matches to best topic, we try to see if, by incrementing n_times the window size, the result for that topic changes more than a given threshold.\n",
    "# If it changes more than a given threhsold at least one time, that best topic is no more considered valid\n",
    "# 6) if the topic is no more considered valid, it restarts from window_dim equal to 1 but this time it starts to search considering only from second best topic to below.\n",
    "# 7) repeats steps 5 and 6 until it find a match that works.\n",
    "# 8) if it find a match that works, we start a new search by moving the window by 1 and comparing all the results obtained with the current best. If a better one is found, substitute the\n",
    "# the best window with the one found, otherwise continue to move until the window no more contain the words of the best window.\n",
    "# 9) finally, if for a word we don't have a match, the algorithm moves to the next word\n",
    "# TODO add a full stop control\n",
    "\n",
    "def process_results_best_policy(results, all_refs, new_th=0 ,window_max_dim=10): #the maximum number of words that the window can contain\n",
    "\n",
    "    #takes all elements of the sentence that have a value greater than threshold\n",
    "    def take_elements_th(el, window_dim, new_th):\n",
    "        #print(el)\n",
    "        all_values = el[f\"window {window_dim}\"]['value']  #all the values (results of the matches between topics and parts of the sentence\n",
    "        th_indexes = [i for i in range(len(all_values)) if all_values[i] > new_th] #returns the indexes of the values that are greater than new_th\n",
    "        values = list(map(all_values.__getitem__,th_indexes)) #take all the values greater than new_th\n",
    "        refs = list(map(el[f\"window {window_dim}\"]['ref'].__getitem__,th_indexes)) #take all the references that return a value greater than new_th\n",
    "        matching_words = list(map(el[f\"window {window_dim}\"]['sentence'].__getitem__,th_indexes)) #take all the sentence parts that return a value greater than new_th\n",
    "        start_word_indexes = list(map(el[f\"window {window_dim}\"]['start_word_indexes'].__getitem__,th_indexes))\n",
    "\n",
    "        return values, refs, matching_words, start_word_indexes\n",
    "\n",
    "    #n_tries indicates how many times we want to check that the threshold in maintained trough time\n",
    "    # t_res contains the result referred to the best topic found in a part of the sentence\n",
    "    # s_res contains all the results obtained by applying the sentence model over a sentence (only results greater tan a certain threshold are taken)\n",
    "    # sentence contains the sentence that we are processing\n",
    "    # max_dim is the maximum size of the window that we can apply over that sentence\n",
    "    def is_coherent(n_tries, th, s_res, t_res, sentence, max_dim):\n",
    "        s_splits = sentence.split()\n",
    "        current_topic = t_res['topic'] #the topic found\n",
    "        current_topic_value = t_res['best_match'] #the value referred to the topic found\n",
    "        end_index = t_res['new_last_word_idx'] + 1 #the index of the last word of the topic found\n",
    "        matching_words = t_res['matching_words'] # a sentence can have more matching_words inside, we have to pick the right ones\n",
    "        start_index = t_res['old_last_word_idx'] + 1\n",
    "        if t_res['new_last_word_idx'] == len(s_splits) - 1: #in this case we reached the end and we have nothing else to check (we assume that is coherent)\n",
    "            return True\n",
    "        #print(\"IS COHERENT:\",matching_words, s_splits[start_index:end_index])\n",
    "        #print(\"Sentence:\",sentence)\n",
    "        #print(\"t_res\", t_res['topic'], t_res['best_match'], t_res['matching_words'],\"start\", start_index,\"end\", end_index,\"len\", len(matching_words),\"ind\", t_res['new_last_word_idx'])\n",
    "        #print(\"COHERENCE\",matching_words, s_splits[start_index:end_index])\n",
    "        assert(matching_words == ' '.join(s_splits[start_index:end_index])) #check if the words we are picking are the same of the matching\n",
    "        topic_ok = False\n",
    "\n",
    "        for t in range(1,n_tries+1):\n",
    "            if end_index + t > len(s_splits) - 1 or len(matching_words.split()) + t > max_dim: #check if, by increasing the window size, we reach the end of the sentence or if we can\n",
    "                return True                                                                    #try to search results in a greater window\n",
    "            else:\n",
    "                window_dim = len(matching_words.split()) + t\n",
    "                s_results = s_res[f\"window {window_dim}\"]\n",
    "                processed_words = ' '.join(s_splits[start_index:(start_index + window_dim)])\n",
    "                #print(\"IS COHERENT 2:\",t_res['old_match_idx'])\n",
    "                start_search_index = t_res['old_match_idx'][f\"window {window_dim}\"] #since we are assuming that the match is still not valid, start to search values from old match_indexes\n",
    "                #print(\"IS COHERENT 3:\",processed_words, s_results['matching_words'][start_search_index:])\n",
    "                if not processed_words in s_results['matching_words'][start_search_index:]: #then the topic doesn't exists anymore, so fur sure we have a drop in probabilities\n",
    "                    return False\n",
    "                else:\n",
    "                    match_index = s_results['matching_words'][start_search_index:].index(processed_words)\n",
    "                    n_possible_topics = 0\n",
    "                    while s_results['matching_words'][start_search_index:][match_index:][n_possible_topics] == processed_words: #count how many topics match with processed words\n",
    "                        n_possible_topics = n_possible_topics + 1\n",
    "                        if n_possible_topics >= len(s_results['matching_words'][start_search_index:][match_index:]):\n",
    "                            break\n",
    "                    all_match_values = s_results['values'][start_search_index:][match_index : match_index + n_possible_topics]\n",
    "                    for value in all_match_values:\n",
    "                        tmp_ref = s_results['refs'][start_search_index:][s_results['values'][start_search_index:].index(value)] #extract the reference associated with the value\n",
    "                        tmp_topic = [key for key in all_refs.keys() for ref in all_refs[key] if tmp_ref == ref][0] #extract the topic associated with reference\n",
    "                        if tmp_topic == current_topic:\n",
    "                            if current_topic_value - value < th: #then the topic does not change so mach by incrementing the window size, check for another n_try\n",
    "                                topic_ok = True\n",
    "                                break\n",
    "                        else: #check the other topic\n",
    "                            continue\n",
    "                    if topic_ok:\n",
    "                        continue\n",
    "                    else:\n",
    "                        return False\n",
    "        return True #if we reached this point than everything is ok. If the function returns False, than we have to avoid the topic\n",
    "\n",
    "    #this function checks if the topic found is for sure the best one: since we didn't try to search results that starts after a given word, we are not sure for that\n",
    "    #the paramters are the same as the previous function except for n_tries\n",
    "    #the function returns a dictionary which tells if the current topic is the best one; if it is the best one that dictionary contain that topic otherwise\n",
    "    #it will contain the best topic (i.e. the topic that returns greater results). The dictionary contains also the skipped words in case we the best topic changes by sliding the window\n",
    "    def is_real_best(s_res, t_res, sentence, max_dim):\n",
    "        s_splits = sentence.split()\n",
    "        max_th_drop = 0.2 #for coherence check\n",
    "        current_topic = t_res['topic'] #the topic found\n",
    "        current_topic_value = t_res['best_match'] #the value referred to the topic found\n",
    "        end_index = t_res['new_last_word_idx'] + 1  #the index of the last word of the topic found\n",
    "        matching_words = t_res['matching_words'] # a sentence can have more matching_words inside, we have to pick the right ones\n",
    "        start_index = t_res['old_last_word_idx'] + 1\n",
    "        skipped_words = ''\n",
    "        end = False\n",
    "        #tmp_new_match_idx = t_res['old_match_idx'].copy()\n",
    "        if t_res['new_last_word_idx'] == len(s_splits) - 1: #in this case we reached the end and we have nothing else to check (we assume that is the best topic)\n",
    "            return {\"is_best\": True, \"topic_result\": t_res, \"skipped_words\":skipped_words}\n",
    "        assert(matching_words == ' '.join(s_splits[start_index:end_index])) #check if the words we are picking are the same of the matching\n",
    "        tmp_best_res = t_res.copy()\n",
    "        #processed_words = ' '.join(s_splits[start_index:(start_index + 1 + window_dim)])\n",
    "        if len(matching_words.split()) == 1: #in this case we have nothing to check since in the next iteration we will search for other words that will not contains the current one\n",
    "            return {\"is_best\": True, \"topic_result\": t_res, \"skipped_words\":skipped_words}\n",
    "        else:\n",
    "            topics_to_avoid = {'topic & dim': []}\n",
    "            tmp_topics_to_avoid =  topics_to_avoid.copy()\n",
    "            for t in range(1,len(matching_words.split())-1):\n",
    "                '''\n",
    "                w_d = 1 #window dimension ############################### this block is used to start to search results from the right index\n",
    "                while t_res['old_last_word_idx'] + w_d <= len(s_splits) - 1 and w_d <= max_dim:    #until we reach the end of the sentence\n",
    "                    s_results = s_res[f\"window {w_d}\"]\n",
    "                    if len(s_results['start_word_indexes']) != 0:\n",
    "                        tmp = [i for i in range(len(s_results['start_word_indexes'])) if s_results['start_word_indexes'][i] > t_res['old_last_word_idx'] + w_d - 1]\n",
    "                        if len(tmp) > 0: #it means that we have at least one value that will possibly match\n",
    "                            tmp_idx_start_match_idx[f\"window {w_d}\"] = tmp[0]\n",
    "                        else:\n",
    "                            tmp_idx_start_match_idx[f\"window {w_d}\"] = len(s_results['matching_words']) #we can avoid to search again\n",
    "                        w_d = w_d + 1\n",
    "                '''\n",
    "                while True:  #try to search if there exists a topic that is coherent (does not change by th if we increase the window) and that is better than the current one\n",
    "                    next_result = find_best_topic(s_res, max_dim, t_res['old_match_idx'], start_index + t, sentence, topics_to_avoid = topics_to_avoid)\n",
    "                    print(next_result)\n",
    "                    if next_result['new_last_word_idx'] != next_result['old_last_word_idx']: ## then we have a new match\n",
    "                        #then this topic associated to these words is not coherent, check the next word\n",
    "                        if not is_coherent(n_tries = 1, th = max_th_drop, s_res = s_res, t_res = next_result, sentence = sentence, max_dim = max_dim): #skip this topic and try to search the next\n",
    "                            topics_to_avoid['topic & dim'].append((next_result['topic'],next_result['best_window']))\n",
    "                            continue\n",
    "                        if next_result['best_match'] > current_topic_value:\n",
    "                            tmp_best_res = next_result.copy()\n",
    "                            skipped_words = s_splits[start_index : start_index + t]\n",
    "                            topics_to_avoid['topic & dim'] = []\n",
    "                        break #in any case we stop the topic search\n",
    "                    if topics_to_avoid != tmp_topics_to_avoid: #then we have a new topic to avoid and we can search if there are som other topics better than the current one\n",
    "                        tmp_topics_to_avoid = topics_to_avoid.copy()\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                    #if next_result['end']: #we reached the end and we didn't find any result\n",
    "                    #    break\n",
    "\n",
    "        return {\"is_best\": False, \"topic_result\": tmp_best_res, \"skipped_words\":skipped_words}\n",
    "\n",
    "    #this function finds the best topic starting from a given word. The best topic is the topic that, for a certain window_dim that contains some of the words of the sentence,\n",
    "    #returns the best results. If for a given word we can't find any match, the function returns some default values, otherwise it retuns the best topic, the associated\n",
    "    #reference and value, the words of the sentence that return the best topic, the window_dim that contains those words, the old and new indexes that tell us where to start\n",
    "    #at the next iteration for both the sentence and the results related to that sentence. If this function is called and we fund that the returned result is for real the best one (for\n",
    "    #some define policy) then we update all the indexes, otherwise we maintain the old ones and we restart teh search by not considering the topics that do not respect the policy.\n",
    "    def find_best_topic(s_res, max_dim, idx_last_matches, idx_last_word, #starting from idx_last_word+1, it will find the window that returns the best topic value\n",
    "                        sentence, topics_to_avoid ={}): #topics_to_avoid indicates which topics we can't search for (e.g. if we found that a topic does not respect the                                                 #policy). and the referred window_dim on which we can't search for that topic\n",
    "        s_splits = sentence.split()\n",
    "        best_match = 0 #initialize the results in case we do not find anything\n",
    "        best_window = 1\n",
    "        ref = ''\n",
    "        topic = ''\n",
    "        matching_words = ''\n",
    "        tmp_best_value_match = 0\n",
    "\n",
    "        tmp_idx_last_matches = idx_last_matches.copy()\n",
    "        #print(idx_last_word)\n",
    "        #print(idx_last_matches)\n",
    "        tmp_idx_last_word = idx_last_word\n",
    "        window_dim = 1\n",
    "        end = False #this becomes true when we reached the end of the sentence, i.e. if we are in the last word\n",
    "\n",
    "        if idx_last_word + window_dim == len(s_splits):\n",
    "            end = True\n",
    "\n",
    "        #print(s_res)\n",
    "        #print(max_dim)\n",
    "        while idx_last_word + window_dim <= len(s_splits) - 1:    #until we reach the end of the sentence\n",
    "            #for window in range(1,max_dim + 1):\n",
    "            s_results = s_res[f\"window {window_dim}\"]\n",
    "            #print(idx_last_matches)\n",
    "            start_search_index = idx_last_matches[f\"window {window_dim}\"] #we take the data inside s_res from this index since the previous ones were already used for search purposes\n",
    "            processed_words = ' '.join(s_splits[(idx_last_word+1):(idx_last_word + 1 + window_dim)]) #we start with window = 1\n",
    "            #print(processed_words, \"    idx last match\", idx_last_matches[f\"window {window_dim}\"], \"    len results\",len(s_results['matching_words']))\n",
    "            #print(\"window_dim:\",window_dim)\n",
    "            #print(\"processed words:\",processed_words, \"  window_dim:\", window_dim, \"  dim_max:\",max_dim, \"  start_search_index:\",start_search_index)\n",
    "            #print(\"RESULTS:\", s_results['matching_words'][start_search_index:], \"window_dim:\", window_dim, processed_words)\n",
    "            if idx_last_matches[f\"window {window_dim}\"] > (len(s_results['matching_words']) - 1): #it means that we have no more results for that window_dim, check with another window\n",
    "                #print(\"continuing...\", \"   window_dim\",window_dim, \"  idx\", idx_last_matches[f\"window {window_dim}\"])\n",
    "                if max_dim > window_dim:\n",
    "                    #print(\"HERE\",max_dim,window_dim)\n",
    "                    window_dim = window_dim + 1\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            #print(processed_words, s_results['matching_words'][start_search_index:])\n",
    "            if processed_words in s_results['matching_words'][start_search_index:]: #then we have at least one match\n",
    "                index = s_results['matching_words'][start_search_index:].index(processed_words) # takes the index of the first match between a topic\n",
    "                                                                                                 # and processed words starting from the last index where we had a match previously\n",
    "                #print(\"index\",index)\n",
    "                n_possible_topics = 0\n",
    "                #print(\"HERE:\", s_results['matching_words'][start_search_index:][index:], processed_words)\n",
    "                while s_results['matching_words'][start_search_index:][index:][n_possible_topics] == processed_words: #count how many topics match withe processed words\n",
    "                    n_possible_topics = n_possible_topics + 1\n",
    "                    #print(n_possible_topics)\n",
    "                    if n_possible_topics >= len(s_results['matching_words'][start_search_index:][index:]): #avoid taking elements that are outside the array\n",
    "                        break\n",
    "                # these values are associated with the topics that match the # processed words\n",
    "                all_match_values = s_results['values'][start_search_index:][index : index + n_possible_topics]\n",
    "                ordered_values = sorted(all_match_values, reverse=True) #order the values that match from the highest to the lowest\n",
    "                #print(ordered_values,tmp_best_value_match)\n",
    "\n",
    "                #tmp_idx_last_matches[f\"window {window_dim}\"] = start_search_index + index + len(ordered_values) - 1\n",
    "                #print(\"inside fun indx matches:\",idx_last_matches)\n",
    "                #print(\"topics to avoid\",topics_to_avoid,\"len ordered values\", len(ordered_values))\n",
    "                #if len(ordered_values) == len(topics_to_avoid): #then we increment the window_size and try with other topics\n",
    "                #    skip_dims.append(window_dim) #we skip until this dimension next iteration\n",
    "                #    print(\"topics before breaking:\",topics_to_avoid)\n",
    "                #    print(\"breaking... Skipping_dim:\",skip_dims)\n",
    "                #    break\n",
    "                for i in range(len(ordered_values)):\n",
    "                    tmp_match = ordered_values[i]\n",
    "                    #print(tmp_match)\n",
    "                    #print(\"match:\",tmp_best_match,\"  window_dim:\",window_dim, \"  start_search_index\", start_search_index)\n",
    "                    tmp_ref = s_results['refs'][start_search_index:][s_results['values'][start_search_index:].index(tmp_match)] #reference associated to best result\n",
    "                    tmp_topic = [key for key in all_refs.keys() for ref in all_refs[key] if tmp_ref == ref][0] #extract the topic associated with reference\n",
    "                    #print(\"FINAL CHECK:\",(tmp_topic,window_dim), \"FINAL CHECK 2:\",topics_to_avoid['topic & dim'])\n",
    "                    if (tmp_topic,window_dim) in topics_to_avoid['topic & dim']: #try with the next ordered value because this one referred to window_dim does not respect he policy\n",
    "                        #print(\"SKIPPED TOPIC\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        if tmp_match > tmp_best_value_match: #then save the current best\n",
    "                            #print(\"OKOKOK\")\n",
    "                            tmp_idx_last_word = idx_last_word + window_dim #Index of the last processed word.\n",
    "                            tmp_best_value_match = tmp_match\n",
    "                            matching_words = processed_words\n",
    "                            best_window = window_dim\n",
    "                            best_match = tmp_best_value_match\n",
    "                            ref = tmp_ref\n",
    "                            topic = tmp_topic\n",
    "                            break # stop the search for this window_dim, try with a bigger window\n",
    "\n",
    "            if max_dim > window_dim:\n",
    "                window_dim = window_dim + 1\n",
    "            else: #we dindn't find any result even with the maximum window size, so we didn't find any match for a word and we can try with the next one (in the main)\n",
    "                break\n",
    "\n",
    "        # save the new indexes from which to start the search for each window. These are temporary since it may happen that the best\n",
    "        # topic found does not respect the policy (in this case indexes must not be updated).\n",
    "        # We search for the first match that has a \"last_word_index\" greater than tmp_idx_last_word ([tmp_idx_last_word + 1][0). Since start index starts from\n",
    "        # idx_last_word + 1, here we have to compensate by subtracting 1\n",
    "        window_dim = 1\n",
    "        while idx_last_word + window_dim <= len(s_splits) - 1 and window_dim <= max_dim:    #until we reach the end of the sentence\n",
    "            s_results = s_res[f\"window {window_dim}\"]\n",
    "            if len(s_results['start_word_indexes']) != 0:\n",
    "                #print(\"IDX LAST WORD:\",tmp_idx_last_word)\n",
    "                tmp = [i for i in range(len(s_results['start_word_indexes'])) if s_results['start_word_indexes'][i] > tmp_idx_last_word]#[0] # - 1\n",
    "                #print(f\"TMP INDEXES DIM :{window_dim}, RES:\", tmp_idx_last_matches[f\"window {window_dim}\"])\n",
    "                if len(tmp) > 0: #it means that we have at least one value that will possibly match\n",
    "                    tmp_idx_last_matches[f\"window {window_dim}\"] = tmp[0]\n",
    "                else:\n",
    "                    tmp_idx_last_matches[f\"window {window_dim}\"] = len(s_results['matching_words']) #we can avoid to search again\n",
    "            window_dim = window_dim + 1\n",
    "\n",
    "        #print(best_match,tmp_idx_last_word,idx_last_word )\n",
    "        return {'best_match':best_match, 'best_window':best_window, 'reference':ref, 'topic':topic, 'matching_words':matching_words, 'new_match_idx':tmp_idx_last_matches,\n",
    "                'old_match_idx':idx_last_matches, 'new_last_word_idx':tmp_idx_last_word, 'old_last_word_idx':idx_last_word, 'end': end, 'topics_to_avoid': topics_to_avoid}\n",
    "\n",
    "    all_labels = [] #it will contain all the labels for all the sentences\n",
    "    max_th_drop = 0.2  #if we find that the best topic x does not change by this amount for n_tries times, then this is the best topic. Otherwise we can try by taking the second best topic and by repeating the check\n",
    "    #all_res = []\n",
    "    for ind,el in enumerate(results):\n",
    "        sentence_labels = {'sentence':'', 'topics&ref':[],  'matching_words':[], 'values':[], \"skipped_words\":[]} #it contains a sentence labelled\n",
    "        sentence = el['whole sentence,speaker,turn'][0]\n",
    "        speaker = el['whole sentence,speaker,turn'][1]\n",
    "        turn = el['whole sentence,speaker,turn'][2]\n",
    "\n",
    "        sentence_labels['sentence'] = sentence\n",
    "        sentence_splits = sentence.split()\n",
    "        final_word_index = len(sentence_splits) - 1 #index of the last word (starting from 0)\n",
    "        last_processed_word_idx = -1\n",
    "        processed_words = sentence_splits[0]\n",
    "        sentence_results = {}\n",
    "        if len(sentence_splits) < window_max_dim:\n",
    "            window_max_d = len(sentence_splits)\n",
    "        else:\n",
    "            window_max_d = window_max_dim\n",
    "\n",
    "        last_match_indexes = {}\n",
    "        #for w in sentence_results.keys():\n",
    "        #    print(sentence_results[w],\"\\n \\n \\n\")\n",
    "        for i in range(1,window_max_d + 1): #take all the data related to the matches greater than a threshold for a given sentence\n",
    "            values, refs, matching_words, start_word_indexes = take_elements_th(el, i, new_th)\n",
    "            sentence_results[f\"window {i}\"] = {'matching_words':[], 'refs':[], 'values':[]}\n",
    "            sentence_results[f\"window {i}\"]['values'] = values\n",
    "            sentence_results[f\"window {i}\"]['matching_words'] = matching_words\n",
    "            sentence_results[f\"window {i}\"]['refs'] = refs\n",
    "            sentence_results[f\"window {i}\"]['start_word_indexes'] = start_word_indexes\n",
    "            last_match_indexes[f\"window {i}\"] = 0 #this dict will contain, for each window_dim, the index of the first element from which we will start the search in that window\n",
    "                                                  #every time we use data, we update this indexes to avoid searching data that is no more useful\n",
    "\n",
    "        topics_to_avoid = {'topic & dim': []}\n",
    "\n",
    "        counter = 0\n",
    "        '''\n",
    "        print(sentence,\"\\n \\n \\n\")\n",
    "        for w in sentence_results.keys():\n",
    "            print(sentence_results[w],\"\\n \\n \\n\")\n",
    "        '''\n",
    "        while True and counter<10:\n",
    "            #print(\"counter:\",counter)\n",
    "            #print(\"topics_before:\",topics_to_avoid)\n",
    "            #print(\"last match idx before:\",last_match_indexes)\n",
    "            result_dict = find_best_topic(sentence_results, window_max_d, last_match_indexes, last_processed_word_idx, sentence, topics_to_avoid)\n",
    "            #print(\"last_idx:\",result_dict['old_last_word_idx'],'new_idx:',result_dict['new_last_word_idx'], result_dict['topic'])\n",
    "            #print(last_processed_word_idx)\n",
    "            #print(result_dict)\n",
    "            if result_dict['new_last_word_idx'] != result_dict['old_last_word_idx']: #then we found a match that respects the policy starting from a word\n",
    "                #print(result_dict, \"\\n \\n \\n\")\n",
    "                #print(is_coherent(n_tries = 1, th = max_th_drop, s_res = sentence_results, t_res = result_dict, sentence = sentence, max_dim = window_max_d))\n",
    "                if is_coherent(n_tries = 1, th = max_th_drop, s_res = sentence_results, t_res = result_dict, sentence = sentence, max_dim = window_max_d):\n",
    "                    #print(\"OK:\",result_dict,\"\\n \\n \\n\")\n",
    "                    #print(\"LAST WORD IDX\",last_processed_word_idx)\n",
    "                    check = is_real_best(s_res = sentence_results, t_res = result_dict, sentence = sentence, max_dim = window_max_d)\n",
    "                    if check[\"is_best\"]: #then the current topic is for real the best one\n",
    "                        sentence_labels['topics&ref'].append((result_dict['topic'],result_dict['reference']))\n",
    "                        sentence_labels['matching_words'].append(result_dict['matching_words'])\n",
    "                        sentence_labels['values'].append(result_dict['best_match'])\n",
    "                        last_processed_word_idx = result_dict['new_last_word_idx']\n",
    "                        last_match_indexes = result_dict['new_match_idx'].copy()\n",
    "                    else:\n",
    "                        new_topic = check[\"topic_result\"]\n",
    "                        sentence_labels['topics&ref'].append((new_topic['topic'],new_topic['reference']))\n",
    "                        sentence_labels['matching_words'].append(new_topic['matching_words'])\n",
    "                        sentence_labels['values'].append(new_topic['best_match'])\n",
    "                        sentence_labels['skipped_words'].append(check['skipped_words']) #TODO manage the skipped words\n",
    "                        last_processed_word_idx = new_topic['new_last_word_idx']\n",
    "                        last_match_indexes = new_topic['new_match_idx'].copy()\n",
    "                        #print(\"HERERHRHERHE\"\n",
    "                    topics_to_avoid['topic & dim'] = [] #in botch cases we found the best result\n",
    "                    #print(\"LAST WORD IDX\",last_processed_word_idx)\n",
    "                else:\n",
    "                    #print(\"PROBLEM:\",result_dict,\"\\n \")\n",
    "                    #print(\"last match idx after:\",last_match_indexes)\n",
    "                    topics_to_avoid['topic & dim'].append((result_dict['topic'],result_dict['best_window']))\n",
    "                    #print(\"NEW TOPICS TO AVOID:\",topics_to_avoid, \"\\n \\n \\n\")\n",
    "                    #print(\"topics_to_append:\",result_dict['topic'])\n",
    "                    #print(\"topics_before:\",topics_to_avoid)\n",
    "\n",
    "                    #print(\"topics_after:\",topics_to_avoid)\n",
    "\n",
    "            #in this case we didn't find any match, go to the next word\n",
    "            else:\n",
    "                topics_to_avoid['topic & dim'] = []\n",
    "                last_processed_word_idx = last_processed_word_idx + 1\n",
    "\n",
    "            if result_dict['end'] == True: #we reached the end\n",
    "                #print(\"ending...\")\n",
    "                break\n",
    "            counter = counter + 1\n",
    "\n",
    "        all_labels.append(sentence_labels)\n",
    "\n",
    "    return all_labels\n",
    "\n",
    "\n",
    "labels = process_results_best_policy([results[1]], all_refs, new_th=0.4)\n",
    "for el in labels:\n",
    "    print(el,\"\\n \\n \\n\")\n",
    "#print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "##### labelling\n",
    "path = r\"C:\\Users\\ariel\\Documents\\PHD\\GitHub\\semantic_similarity\\notebooks\"\n",
    "filename_best = \"new_all_processed_sentences_best.pkl\"\n",
    "\n",
    "if(os.path.exists(os.path.join(path,filename_best))):    #load data already processed\n",
    "    results = pickle.load(open(os.path.join(path,filename_best),'rb'))\n",
    "else:\n",
    "    print(\"Invalid path\")\n",
    "#print(all_refs.keys())\n",
    "\n",
    "all_refs = {\"r_greet\":reference_sentence_greet, \"r_faster\":reference_sentence_faster, \"r_goaway\":reference_sentence_goaway,\n",
    "           \"r_cigarette\":reference_sentence_cigarette, \"r_idk\":reference_sentence_idk, \"r_no\":reference_sentence_no,\n",
    "           \"r_yes\":reference_sentence_yes, \"r_deitic\":reference_sentence_deitic,\"r_run\":reference_sentence_run,\n",
    "           \"r_stop\":reference_sentence_stop, \"r_adaptor\":reference_sentence_adaptor, \"r_telephone\":reference_sentence_telephone,\n",
    "           \"r_come\":reference_sentence_come, \"r_drink\":reference_sentence_drink, \"r_exulting\":reference_sentence_exulting,\n",
    "            \"r_link\":reference_sentence_link, \"r_apologize\":reference_sentence_apologize, \"r_attention\":reference_sentence_attention,\n",
    "            \"r_beg\":reference_sentence_beg, \"r_careless\":reference_sentence_careless, \"r_impossible\":reference_sentence_impossible,\n",
    "            \"r_walk\":reference_sentence_walk,\"r_thin\":reference_sentence_thin, \"r_later\":reference_sentence_later,\n",
    "            \"r_lot\":reference_sentence_lot, \"r_time\":reference_sentence_time, \"r_clap\":reference_sentence_clap,\"r_ita\":reference_sentence_ita}\n",
    "\n",
    "topic-gesture_description = {\"Greeting\":(\"r_greet\", \"the gesture that you use when you greet people\"), \"Talk faster\":(\"r_faster\", \"the gesture that you use when you want someone to\"\n",
    "                        \"talk faster (for example by keeping one or two hands ahead, palms facing you and wrist rotating them recursively)\"), \"Go away\":(\"r_goaway\", \"the\"\n",
    "                        \"gesture that you use when you want someone to go away from you (for example by keeping one or two hands ahead, palms facing you and by moving the hand back and forth\"\n",
    "                        \"starting from the palm facing down)\"), \"Cigarette\":(\"r_cigarette\",\"the gesture that you do when you want to show smoking intentions (bring two fingers\"\n",
    "                        \"close to the mouth)\"), \"I don't know\":(\"r_idk\", \"moving the shoulders up and down or move both the arms in front of you with palms facing up, used when you want to\"\n",
    "                        \"to show that you don't know something\"), \"No\":(\"r_no\", \"moving the head or a finger left and right when you want to say no\"), \"Yes\":(\"moving the head up and\" \\\n",
    "                        \"down when you are agreeing for something or you want to say yes\"), \"Pointing something\":(\"r_deitic\",\"the gesture that you use for indicating something (with head, finger, etc.)\"),\n",
    "                        \"Pointing yourself\":(\"r_adaptor\",\"the gesture that you use to point yourself or a part of your body\"), \"Run\": (\"r_run\",\"moving arms aback and forth while\" \\\n",
    "                        \"maintaining an angle of 90 with the elbow to simulate a running movement\"), \"Walk\":(\"r_walk\",\"moving the index and the middle fingers back and forth to simulate\" \\\n",
    "                        \"a walking movement\"), \"Stop\":(\"r_stop\",\"gesture that you make with arms to tell someone that he as to slow down or stop to do something (for example with arms ahead,\" \\\n",
    "                        \"back of the hand facing you and while moving back and forth)\"), \"Call\":(\"r_telephone\", \"the gesture that you make to simulate a phone call, for instance by keeping\" \\\n",
    "                        \"up the thumb and pinky fingers of one hand and by bringing the hand close to an ear\"), \"Come to me\":(\"r_come\", \"the gesture that you make to someone to tell him/her\" \\\n",
    "                        \"to get close, for instance by keeping an arm straight, palm facing down and my moving banck and forth the fingers\"), \"Drinking\":(\"r_drink\", \"the gesture that you make\" \\\n",
    "                        \"to simulate someone drinking, for instance by keeping up the thumb and pinky fingers of one hand and by bringing the hand close to the mouth\"), \"Exulting\":(\"r_exulting\",\n",
    "                        \"the gesture that you make when you are exulting (for example by putting both your arms up and by putting your keeping your hands in the shape of a fist)\"), \"They are related\":\n",
    "                        \"the gesture that you make to tell that 2 people or objects are related (for example by making 2 fists and by bringing them close to each other)\"), \"Apologize\":(\"r_apologize\",\n",
    "                        \"the gesture that you make when you ask someone to forgive you, for instance by tilting the upper body or by keeping arms up, palms facing up and elbows bent so that they\" \\\n",
    "                        \"form a 90 angle\"), \"Attention please\":(\"r_attention\",\"the gesture that you make when you ask attention to a person or group of people, for instance by keeping up the\" \\\n",
    "                        \" index while in front of you\"), \"Begging you\":(\"the gesture that you make when you are begging someone, for instance by keeping arms in a pray position\"), \"I don't care\":\n",
    "                        \"the gesture that you make when you are not interested in something (for example by bringing the back of one hand under the chin and by moving it back and forth\"),\n",
    "                        \"It's impossible\": (\"r_impossible\",\"the gesture that you make when you don't believe that something is possible, for instance by bending one elbow in a 90 \" \\\n",
    "                        \"angle, palm of that arm facing up, and by keeping up only the index and the thumb fingers\"), \"It's thin\": (\"r_thin\",\"keeping only the pinkie of one hand up while maintaining the other\" \\\n",
    "                        \"indexes down and the elbow bent to say that something is thin\"), \"I'll do it later\": (\"r_later\", \"keeping a hand in front of you with only the index up to communicate to someone\" \\\n",
    "                        \"that somthign will happen later and not now\"), \"It's a lot\":(\"r_lot\":\"the gesture that you make when you think that something is a lot, for example by keeping a hand in front of you\" \\\n",
    "                        \"with palm facing you, and by moving it up and down\"), \"It's late\":(\"r_time\", \"indicating the wrist with an index to say that it's late for something\"), \"Congratulations\": (\"r_clap\",\n",
    "                        \"clapping with hands to congratulate with someone\"), \"What are you saying?\":(\"r_ita\", \"keep one hand in front of you, palm facing you, the thumb is over the other fingers and the hand\" \\\n",
    "                        \"is moving back and forth. Used, especially in italy, to tell someone that what is he saying does not make any sense\")}\n",
    "\n",
    "print(\"In the next there will be shown to you some sentences. Imagine that you are saying the words in the sentence and to gesticulate while saying them. Label the sentences by writing\"\n",
    "      \"down the words and the gesture that you imagine to do while saying those words. You can only choose among the following gestures \")\n",
    "for res in results:\n",
    "    print(\"\")\n",
    "    #res['whole sentence,speaker,turn'][0]\n",
    "nome = input(all_refs.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = {'f':[1,2],'g':[3,4]}\n",
    "b = a\n",
    "b['f'][0]=6\n",
    "print(a, b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = {'f':{'m':'cao','v':2},'g':{'m':'cio','v':4},'p':{'m':'ciao','v':1}}\n",
    "elements = a.items()\n",
    "print(list(a.keys()),list(elements))\n",
    "#print([x for x in list(a.keys())])\n",
    "mmm = map(lambda el: el[1]['v'], list(elements))\n",
    "print(list(mmm))\n",
    "\n",
    "#sort_a = sorted(a.items(), key=lambda x: x[1]['v'], reverse=True)\n",
    "#print(sort_a)\n",
    "#print(a,\"\\n\",sorted(a,key=a['v']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_results_best_policy(result, all_refs, new_th=0):\n",
    "    #print(len(result))\n",
    "    all_labels = [] #it contains all the labels for all the sentences\n",
    "    n_tries = 1 #after we find the best matching topic for a set of words, we check if the topic changes by incrementing the window_size: If n_tries is = 2, then the\n",
    "                #best topic must remain the same for 2 times. For instance, if we have the sentence \"Hi, how are you?\" and we find that the best topic for \"Hi\" is \"greeting\"\n",
    "                #then also for \"Hi, how\" and \"Hi, how are\" the topic must be greeting.\n",
    "    max_n = 2 #it takes the first max_n best topics. If we try to increment the window size and we find that the topic changes more than a given threshold, get the second best topic and repeat the check\n",
    "    mex_th_between_topic = 0.2  #if we find that the best topic x does not change by this amount for n_tries times, then this is the best topic. Otherwise we can try by taking the second best topic and by repeating the check\n",
    "\n",
    "\n",
    "    for el in result:\n",
    "        sentence_labels = {'sentence':'', 'topics&ref':[], 'words':[], 'values':[]} #it contains a sentence labelled\n",
    "        sentence = el['whole sentence,speaker,turn'][0]\n",
    "        speaker = el['whole sentence,speaker,turn'][1]\n",
    "        turn = el['whole sentence,speaker,turn'][2]\n",
    "        sentence_labels['sentence'] = sentence\n",
    "\n",
    "        sentence_splits = sentence.split()\n",
    "        final_word_index = len(sentence_splits)\n",
    "        window_dim = 1 #the size of the window (i.e. number of words). We check if there is a match between these words and the reference sentences\n",
    "        current_words = sentence_splits[0] # We start to analyze the first word, so with window dim = 1\n",
    "        last_current_word_index = 1 #this contain the index of the last word that have analyzed (it is the last word of the set of words that had a match or the next word of a word that didn't have a match\n",
    "        last_current_word_index_break = 1 #this is used in case we do not find any instance to avoid breaking the most external while; it contains the index of the of the last\n",
    "                                          #word that was checked for sentence similarity. We tried all to find a similarity for all the windows but we do not find anything, so\n",
    "                                          #we continue with the next word\n",
    "\n",
    "        index_loop = 1\n",
    "\n",
    "        while last_current_word_index <= final_word_index: # if the condition is false we reached the end and we can analyze the next sentence\n",
    "            #print(current_words,  el[f\"window {window_dim}\"]['sentence'], not current_words in el[f\"window {window_dim}\"]['sentence'])\n",
    "            if not current_words in el[f\"window {window_dim}\"]['sentence']: #the words that we are analyzing do not have any match with references -> increment window_size\n",
    "                #print(\"HERE\", current_words, el[f\"window {window_dim}\"]['sentence'], last_current_word_index)\n",
    "                window_dim = window_dim + 1\n",
    "                if last_current_word_index == final_word_index: #we reached the end of the sentence. Notice that it doesn't mean that we finished the analysis but only to try\n",
    "                                                                #to find a match with a window that contains all the words from a given word to the end. Now we have to try with\n",
    "                                                                #the next word, so with window dim=1\n",
    "                    window_dim = 1 # restart again to check matches from the next word and window_size = 1\n",
    "                    #print(sentence,last_current_word_index_break, current_words)\n",
    "                    if last_current_word_index_break == final_word_index: # in this case we can't try anything more, exit from the main loop and analyze another sentence\n",
    "                        last_current_word_index = last_current_word_index + 1 #this causes the break of the main loop, last_current_word_index is already referred to last word\n",
    "                    else:\n",
    "                        current_words = sentence_splits[last_current_word_index_break]  #restart from next next word\n",
    "                        last_current_word_index_break = last_current_word_index_break + 1 #update the index (now it id referred to the new current word)\n",
    "                        last_current_word_index = last_current_word_index_break #update the index\n",
    "                    continue\n",
    "                last_current_word_index = last_current_word_index + 1 #try by adding a word\n",
    "                current_words = current_words + ' ' + sentence_splits[last_current_word_index-1]\n",
    "                continue\n",
    "            else:\n",
    "                #print(\"HERE2\", current_words)\n",
    "                window_dim_tmp = window_dim\n",
    "                win_results = el[f\"window {window_dim}\"] #all the results that we have for a given window dimension\n",
    "                n_possible_topics = win_results['sentence'].count(current_words) #all the possible topics that returns a result greater than threshold\n",
    "                #print(n_possible_topics )\n",
    "                values = win_results['value']   #all the values referred to the topics that return result greater than a threshold\n",
    "                index = win_results['sentence'].index(current_words) #in the case that we have at least one topic > threshold, we take the first index of topics that match\n",
    "                #print(\"mmmmmm\", values[index : index + n_possible_topics])\n",
    "                #best = max(values[index : index + n_possible_topics]) #score of the topic that returns the best result\n",
    "                all_bests = heapq.nlargest(max_n , values[index : index + n_possible_topics]) #the first max_n best topics\n",
    "                n_max_index = 0\n",
    "                best = all_bests[n_max_index] #the best topic\n",
    "                #print(best)\n",
    "                best_index = values.index(best) #the best topic\n",
    "                ref = win_results['ref'][best_index] #reference associated to best result\n",
    "                #print(ref)\n",
    "                topic = [key for key in all_refs.keys() for tmp_ref in all_refs[key] if ref == tmp_ref][0] #extract the topic associated with reference\n",
    "                #print(topic)\n",
    "                #print(final_word_index)\n",
    "                tmp_last_current_word_index = last_current_word_index #tmp_last_current_word_index contain the index of the last of the words that return the maximum similarity with reference sentences\n",
    "                #print(\"TMP LAST CURRENT WORD\", tmp_last_current_word_index)\n",
    "                words = current_words #the words tha that matched with the topic\n",
    "                while last_current_word_index <= final_word_index :# and index_loop<200: #try to see if, by incrementing the window size, the next best topic returns a greater result.\n",
    "                    #index_loop = index_loop + 1\n",
    "                    #print(\"HERE3\")\n",
    "                    window_dim = window_dim + 1\n",
    "                    last_current_word_index = last_current_word_index + 1\n",
    "                    #print(last_current_word_index)\n",
    "                    if last_current_word_index > final_word_index:  #I will always enter one time in this if. If I enter here, I tried all windows and take the best one\n",
    "                        #print(\"I must not be here\")\n",
    "                        if best > new_th: #avoid to append results that are worse than new_th\n",
    "                            for i in range(n_tries): #check if the topic is the same even if we try bigger window dim\n",
    "                                if not tmp_last_current_word_index < final_word_index: # then we cannot check anymore so we save the best results\n",
    "                                    sentence_labels['topics&ref'].append((topic,ref))\n",
    "                                    sentence_labels['words'].append(words)\n",
    "                                    sentence_labels['values'].append(best)\n",
    "                                    break\n",
    "                                else:\n",
    "                                    current_tmp_words = words + ' ' + sentence_splits[tmp_last_current_word_index]\n",
    "                                    if not current_tmp_words in win_results['sentence']: #then we have not any solution. In this case, take another best match and restart the while loop\n",
    "                                        n_max_index = n_max_index + 1\n",
    "                                        if n_max_index < n_max and n_max_index < len(all_bests): # in this case we can take the next best value in the list of the first n_best_values (n chosen)\n",
    "                                            best = all_bests[n_max_index]\n",
    "                                            window_dim = window_dim_tmp #this is the window dim that we had with\n",
    "                                            win_results = el[f\"window {window_dim}\"]\n",
    "                                            values = win_results['value']\n",
    "                                            best_index = values.index(best)\n",
    "                                            ref = win_results['ref'][best_index]\n",
    "                                            topic = [key for key in all_refs.keys() for tmp_ref in all_refs[key] if ref == tmp_ref][0]\n",
    "                                            last_current_word_index = last_current_word_index_break #restart from where we break\n",
    "                                            tmp_last_current_word_index = last_current_word_index\n",
    "                                            n_max_index = 0\n",
    "                                            break\n",
    "                                    #tmp_last_current_word_index = tmp_last_current_word_index + 1\n",
    "                                    #window_dim = window_dim + 1\n",
    "                                    #values = win_results['value']\n",
    "                                    #n_possible_topics = win_results['sentence'].count(current_tmp_words)\n",
    "                                    #index = win_results['sentence'].index(current_tmp_words)\n",
    "                                    #best_tmp = max(values[index : index + n_possible_topics])\n",
    "                                    if best == best_tmp:\n",
    "                                        continue\n",
    "                                    else:\n",
    "\n",
    "\n",
    "\n",
    "                        #print(\"HERE 4!\", best, tmp_last_current_word_index)\n",
    "                        if tmp_last_current_word_index < final_word_index: #otherwise we cannot continue\n",
    "                            window_dim = 1      #restart the search from the word next to the previous best window that labels the sentence\n",
    "                            current_words = sentence_splits[tmp_last_current_word_index]\n",
    "                            #print(current_words)\n",
    "                        last_current_word_index = tmp_last_current_word_index + 1\n",
    "                        last_current_word_index_break = last_current_word_index\n",
    "                        #print(\"HEREn\",last_current_word_index, current_words)\n",
    "                        break\n",
    "                    else:\n",
    "                        win_results = el[f\"window {window_dim}\"]\n",
    "                        #print(win_results)\n",
    "                        #print(\"HERE 9\")\n",
    "                        #print(\"mmmmmmmmm\")\n",
    "                        current_words = current_words + ' ' + sentence_splits[last_current_word_index-1]\n",
    "                        #print(current_words, win_results['sentence'])\n",
    "                        if not current_words in win_results['sentence']: #then we have not any solution with current words, try by increasing again the window dim\n",
    "                            continue\n",
    "                        values = win_results['value']\n",
    "                        #print(values)\n",
    "                        #print(current_words, win_results['sentence'], last_current_word_index)\n",
    "                        n_possible_topics = win_results['sentence'].count(current_words)\n",
    "                        #print(n_possible_topics)\n",
    "                        index = win_results['sentence'].index(current_words)\n",
    "                        best_tmp = max(values[index : index + n_possible_topics])\n",
    "                        #print(best_tmp, best)\n",
    "                        if best_tmp > best: #in this case we have a better result by increasing window dim\n",
    "                            best = best_tmp\n",
    "                            window_dim_tmp  = window_dim\n",
    "                            best_index = values.index(best)\n",
    "                            ref = win_results['ref'][best_index] #reference associated to best result\n",
    "                            topic = [key for key in all_refs.keys() for tmp_ref in all_refs[key] if ref == tmp_ref][0]\n",
    "                            words = current_words\n",
    "                            tmp_last_current_word_index = last_current_word_index\n",
    "                            #print(\"TMP!!\",tmp_last_current_word_index)\n",
    "                        else:\n",
    "                            continue #try with another window dim\n",
    "        all_labels.append(sentence_labels)\n",
    "    return all_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "gestures",
   "language": "python",
   "display_name": "gestures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
